{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhvGP5vCyfAm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/atharva/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "colab_type": "code",
    "id": "rPisS083zn9t",
    "outputId": "7fc85848-aad9-41cc-d69e-c703d0242ede"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('max_price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>money_left</th>\n",
       "      <th>property_cost</th>\n",
       "      <th>group_properties_needed</th>\n",
       "      <th>opponent_avg</th>\n",
       "      <th>go_dist</th>\n",
       "      <th>opponent_house_distance</th>\n",
       "      <th>max_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1800</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>1800</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1200</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>240</td>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1600</td>\n",
       "      <td>240</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1400</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200</td>\n",
       "      <td>220</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>400</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>600</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>1800</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>600</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1600</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>800</td>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>1200</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>600</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>400</td>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1400</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>400</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1800</td>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1600</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>200</td>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1600</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>800</td>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1600</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1800</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1800</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>400</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1400</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1800</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>1200</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>800</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>800</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>800</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1400</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>800</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>800</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>1200</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1200</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>800</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1600</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>600</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>400</td>\n",
       "      <td>220</td>\n",
       "      <td>2</td>\n",
       "      <td>1800</td>\n",
       "      <td>38</td>\n",
       "      <td>28</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>800</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1000</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1200</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>600</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>600</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1800</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1600</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>1800</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1200</td>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1600</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1200</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1400</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1800</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>400</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>200</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>400</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>1400</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1400</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>400</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     money_left  property_cost  group_properties_needed  opponent_avg  \\\n",
       "0           200            200                        2           800   \n",
       "1          1200             60                        1          1400   \n",
       "2          1800            160                        1          1800   \n",
       "3           400            200                        1          1200   \n",
       "4          1000            240                        4           400   \n",
       "5           200             60                        2           200   \n",
       "6          1600            240                        2           400   \n",
       "7          1400            200                        4             0   \n",
       "8             0            100                        3           200   \n",
       "9           200            220                        2          1000   \n",
       "10          200            100                        1          1600   \n",
       "11          800            300                        2           800   \n",
       "12          400            350                        1          1600   \n",
       "13          600            140                        2          1800   \n",
       "14          600            100                        3           800   \n",
       "15         1600             60                        3           800   \n",
       "16          800            140                        3          1200   \n",
       "17            0             60                        2           600   \n",
       "18          400            140                        3           200   \n",
       "19         1400            120                        2          1000   \n",
       "20          400            180                        3             0   \n",
       "21            0            350                        1             0   \n",
       "22         1800            140                        3           600   \n",
       "23         1600            320                        1           600   \n",
       "24          200            140                        3           400   \n",
       "25         1600            260                        1           800   \n",
       "26          800            240                        1           600   \n",
       "27         1600            100                        1           800   \n",
       "28         1800             60                        2          1800   \n",
       "29          400            260                        2           400   \n",
       "..          ...            ...                      ...           ...   \n",
       "171        1400            200                        1           600   \n",
       "172        1800            180                        3          1200   \n",
       "173         800            350                        1          1600   \n",
       "174         800             60                        1           200   \n",
       "175         800            150                        3           400   \n",
       "176        1400            220                        3           400   \n",
       "177         800            200                        1          1600   \n",
       "178         800            300                        3          1200   \n",
       "179        1200            400                        1           400   \n",
       "180         800            150                        3           800   \n",
       "181        1600            200                        2             0   \n",
       "182         600            180                        1           400   \n",
       "183         400            220                        2          1800   \n",
       "184         800            180                        3          1000   \n",
       "185        1000            400                        2          1200   \n",
       "186        1200            120                        3           400   \n",
       "187         600             60                        1           800   \n",
       "188         600            100                        3           600   \n",
       "189        1000            100                        1          1800   \n",
       "190        1600            220                        3          1800   \n",
       "191        1200            140                        3          1000   \n",
       "192        1600            180                        3          1000   \n",
       "193        1200            260                        2           200   \n",
       "194        1400            200                        1          1800   \n",
       "195         400            300                        3          1000   \n",
       "196         200            140                        2           400   \n",
       "197         400            220                        1          1400   \n",
       "198        1400            200                        2          1200   \n",
       "199         400            260                        2          1200   \n",
       "200         200            180                        3             0   \n",
       "\n",
       "     go_dist  opponent_house_distance  max_price  \n",
       "0         17                       32        120  \n",
       "1         18                       10        250  \n",
       "2         12                       24        350  \n",
       "3         18                        6        450  \n",
       "4         14                       17        240  \n",
       "5         22                        9          0  \n",
       "6          2                       26        240  \n",
       "7         16                       20        200  \n",
       "8          8                        6          0  \n",
       "9         11                       10        100  \n",
       "10        10                       32        150  \n",
       "11        32                       30        300  \n",
       "12        36                       14        400  \n",
       "13         6                       18        160  \n",
       "14        30                       20        100  \n",
       "15        28                       26        150  \n",
       "16        36                        5        150  \n",
       "17        34                       26          0  \n",
       "18        11                       32        150  \n",
       "19        15                        3        190  \n",
       "20         1                       24        400  \n",
       "21        12                        4        400  \n",
       "22         9                        5        200  \n",
       "23        15                       26        550  \n",
       "24         8                       24        100  \n",
       "25         5                        1        500  \n",
       "26        24                        7        280  \n",
       "27         6                       20        300  \n",
       "28        34                       38         80  \n",
       "29        16                        9        200  \n",
       "..       ...                      ...        ...  \n",
       "171        4                       22        360  \n",
       "172       10                        2        200  \n",
       "173        2                        1        500  \n",
       "174        2                       15        140  \n",
       "175       14                        3        150  \n",
       "176        1                       10        240  \n",
       "177       32                       15        400  \n",
       "178       16                        2        200  \n",
       "179       34                        6        490  \n",
       "180        7                        2        160  \n",
       "181        3                       10        220  \n",
       "182       13                       24        300  \n",
       "183       38                       28        100  \n",
       "184       10                       11        150  \n",
       "185        1                       26        420  \n",
       "186       15                       30        130  \n",
       "187       17                       16        190  \n",
       "188       15                       36        120  \n",
       "189       24                       15        300  \n",
       "190       28                       17        240  \n",
       "191        9                       14        180  \n",
       "192       34                        5        210  \n",
       "193       18                       16        220  \n",
       "194       15                       20        450  \n",
       "195        2                        9        130  \n",
       "196       26                       36         60  \n",
       "197       12                       38        300  \n",
       "198       32                       30        270  \n",
       "199       16                        1        260  \n",
       "200        8                       22         90  \n",
       "\n",
       "[201 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Icxo3grSz4vJ"
   },
   "outputs": [],
   "source": [
    "X=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO8wk-VC4JPs"
   },
   "outputs": [],
   "source": [
    "# X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_train=X\n",
    "y_train=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qUi-MZw3gnz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atharva/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardScaleX=StandardScaler()\n",
    "\n",
    "X_train=standardScaleX.fit_transform(X_train)\n",
    "\n",
    "# X_test = standardScaleX.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VwCQaiO5S1q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atharva/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=6, activation=\"relu\", units=16)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/atharva/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16)`\n",
      "  \"\"\"\n",
      "/home/atharva/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16)`\n",
      "  import sys\n",
      "/home/atharva/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1)`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(input_dim=X_train.shape[1],activation=\"relu\",output_dim=16))\n",
    "\n",
    "model.add(Dense(activation=\"relu\",output_dim=16))\n",
    "\n",
    "model.add(Dense(activation=\"relu\",output_dim=16))\n",
    "\n",
    "model.add(Dense(activation=\"linear\",output_dim=1))\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss='mean_absolute_error',metrics=[\"mean_absolute_error\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NiHPCG0R79k2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atharva/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 248.0073 - mean_absolute_error: 248.0073\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 247.7860 - mean_absolute_error: 247.7860\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 92us/step - loss: 247.4501 - mean_absolute_error: 247.4501\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 121us/step - loss: 246.9057 - mean_absolute_error: 246.9057\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 246.0069 - mean_absolute_error: 246.0069\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 244.5935 - mean_absolute_error: 244.5935\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 128us/step - loss: 242.3358 - mean_absolute_error: 242.3358\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 130us/step - loss: 238.8935 - mean_absolute_error: 238.8935\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 116us/step - loss: 233.8329 - mean_absolute_error: 233.8329\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 133us/step - loss: 226.5724 - mean_absolute_error: 226.5724\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 88us/step - loss: 216.6828 - mean_absolute_error: 216.6828\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 134us/step - loss: 203.1689 - mean_absolute_error: 203.1689\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 152us/step - loss: 185.6625 - mean_absolute_error: 185.6625\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 132us/step - loss: 163.4844 - mean_absolute_error: 163.4844\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 209us/step - loss: 138.7828 - mean_absolute_error: 138.7828\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 86us/step - loss: 113.3780 - mean_absolute_error: 113.3780\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 123us/step - loss: 91.3911 - mean_absolute_error: 91.3911\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 77.7866 - mean_absolute_error: 77.7866\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 69.6866 - mean_absolute_error: 69.6866\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 154us/step - loss: 63.7469 - mean_absolute_error: 63.7469\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 59.0878 - mean_absolute_error: 59.0878\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 55.0465 - mean_absolute_error: 55.0465\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 155us/step - loss: 52.5402 - mean_absolute_error: 52.5402\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 50.5997 - mean_absolute_error: 50.5997\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 132us/step - loss: 48.9490 - mean_absolute_error: 48.9490\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 155us/step - loss: 47.8489 - mean_absolute_error: 47.8489\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 142us/step - loss: 46.9800 - mean_absolute_error: 46.9800\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 46.6240 - mean_absolute_error: 46.6240\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 208us/step - loss: 45.7870 - mean_absolute_error: 45.7870\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 128us/step - loss: 45.4735 - mean_absolute_error: 45.4735\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 44.9181 - mean_absolute_error: 44.9181\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 154us/step - loss: 44.6181 - mean_absolute_error: 44.6181\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 162us/step - loss: 44.2307 - mean_absolute_error: 44.2307\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 160us/step - loss: 43.8087 - mean_absolute_error: 43.8087\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 153us/step - loss: 43.5719 - mean_absolute_error: 43.5719\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 43.3172 - mean_absolute_error: 43.3172\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 162us/step - loss: 42.9930 - mean_absolute_error: 42.9930\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 129us/step - loss: 42.8867 - mean_absolute_error: 42.8867\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 42.6132 - mean_absolute_error: 42.6132\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 42.4575 - mean_absolute_error: 42.4575\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 213us/step - loss: 42.5810 - mean_absolute_error: 42.5810\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 42.0259 - mean_absolute_error: 42.0259\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 155us/step - loss: 42.1116 - mean_absolute_error: 42.1116\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 41.7782 - mean_absolute_error: 41.7782\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 41.9160 - mean_absolute_error: 41.9160\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 155us/step - loss: 41.4162 - mean_absolute_error: 41.4162\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 157us/step - loss: 41.3919 - mean_absolute_error: 41.3919\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 138us/step - loss: 41.2651 - mean_absolute_error: 41.2651\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 123us/step - loss: 41.0086 - mean_absolute_error: 41.0086\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 136us/step - loss: 40.9545 - mean_absolute_error: 40.9545\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 155us/step - loss: 40.7661 - mean_absolute_error: 40.7661\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 156us/step - loss: 40.5973 - mean_absolute_error: 40.5973\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 132us/step - loss: 40.4709 - mean_absolute_error: 40.4709\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 40.5875 - mean_absolute_error: 40.5875\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 40.4267 - mean_absolute_error: 40.4267\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 40.2422 - mean_absolute_error: 40.2422\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 40.1921 - mean_absolute_error: 40.1921\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 98us/step - loss: 40.0630 - mean_absolute_error: 40.0630\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 0s 131us/step - loss: 39.8713 - mean_absolute_error: 39.8713\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 127us/step - loss: 39.6755 - mean_absolute_error: 39.6755\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 39.6868 - mean_absolute_error: 39.6868\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 0s 101us/step - loss: 39.5551 - mean_absolute_error: 39.5551\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 39.5328 - mean_absolute_error: 39.5328\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 130us/step - loss: 39.5649 - mean_absolute_error: 39.5649\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 0s 116us/step - loss: 39.2304 - mean_absolute_error: 39.2304\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 39.3674 - mean_absolute_error: 39.3674\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 135us/step - loss: 38.9736 - mean_absolute_error: 38.9736\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 114us/step - loss: 39.0830 - mean_absolute_error: 39.0830\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 39.0623 - mean_absolute_error: 39.0623\n",
      "Epoch 70/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 118us/step - loss: 38.8409 - mean_absolute_error: 38.8409\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 39.0714 - mean_absolute_error: 39.0714\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 131us/step - loss: 39.0185 - mean_absolute_error: 39.0185\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 38.7697 - mean_absolute_error: 38.7697\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 127us/step - loss: 38.8395 - mean_absolute_error: 38.8395\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 105us/step - loss: 38.5783 - mean_absolute_error: 38.5783\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 38.6658 - mean_absolute_error: 38.6658\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 38.6031 - mean_absolute_error: 38.6031\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 38.5290 - mean_absolute_error: 38.5290\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 38.5308 - mean_absolute_error: 38.5308\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 38.7247 - mean_absolute_error: 38.7247\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 38.4426 - mean_absolute_error: 38.4426\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 38.3550 - mean_absolute_error: 38.3550\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 107us/step - loss: 38.3605 - mean_absolute_error: 38.3605\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 38.1769 - mean_absolute_error: 38.1769\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 160us/step - loss: 38.1545 - mean_absolute_error: 38.1545\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 38.0845 - mean_absolute_error: 38.0845\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 38.0810 - mean_absolute_error: 38.0810\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 38.0999 - mean_absolute_error: 38.0999\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 133us/step - loss: 38.0483 - mean_absolute_error: 38.0483\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 114us/step - loss: 37.9533 - mean_absolute_error: 37.9533\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 37.8349 - mean_absolute_error: 37.8349\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 37.9459 - mean_absolute_error: 37.9459\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 37.9097 - mean_absolute_error: 37.9097\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 38.1507 - mean_absolute_error: 38.1507\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 0s 124us/step - loss: 37.7907 - mean_absolute_error: 37.7907\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 0s 88us/step - loss: 37.7191 - mean_absolute_error: 37.7191\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 37.7875 - mean_absolute_error: 37.7875\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 0s 132us/step - loss: 37.6935 - mean_absolute_error: 37.6935\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 0s 101us/step - loss: 37.8412 - mean_absolute_error: 37.8412\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 37.5810 - mean_absolute_error: 37.5810\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 37.7257 - mean_absolute_error: 37.7257\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 0s 101us/step - loss: 37.6311 - mean_absolute_error: 37.6311\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 0s 100us/step - loss: 37.5117 - mean_absolute_error: 37.5117\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 37.5194 - mean_absolute_error: 37.5194\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 37.3813 - mean_absolute_error: 37.3813\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 37.3986 - mean_absolute_error: 37.3986\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 0s 100us/step - loss: 37.4486 - mean_absolute_error: 37.4486\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 37.4346 - mean_absolute_error: 37.4346\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 37.4087 - mean_absolute_error: 37.4087\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 0s 105us/step - loss: 37.3354 - mean_absolute_error: 37.3354\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 37.2079 - mean_absolute_error: 37.2079\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 0s 112us/step - loss: 37.2031 - mean_absolute_error: 37.2031\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 37.2584 - mean_absolute_error: 37.2584\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 0s 105us/step - loss: 37.0957 - mean_absolute_error: 37.0957\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 0s 114us/step - loss: 37.4010 - mean_absolute_error: 37.4010\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 0s 123us/step - loss: 37.0772 - mean_absolute_error: 37.0772\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 37.0992 - mean_absolute_error: 37.0992\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 0s 131us/step - loss: 37.2521 - mean_absolute_error: 37.2521\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 0s 104us/step - loss: 36.9669 - mean_absolute_error: 36.9669\n",
      "Epoch 120/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 37.0006 - mean_absolute_error: 37.0006\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 37.0306 - mean_absolute_error: 37.0306\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 0s 105us/step - loss: 36.8662 - mean_absolute_error: 36.8662\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 36.8402 - mean_absolute_error: 36.8402\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 0s 101us/step - loss: 36.9087 - mean_absolute_error: 36.9087\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 0s 103us/step - loss: 36.9476 - mean_absolute_error: 36.9476\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 36.7859 - mean_absolute_error: 36.7859\n",
      "Epoch 127/500\n",
      "160/160 [==============================] - 0s 97us/step - loss: 36.7641 - mean_absolute_error: 36.7641\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 36.7998 - mean_absolute_error: 36.7998\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 36.8781 - mean_absolute_error: 36.8781\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 36.6085 - mean_absolute_error: 36.6085\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 0s 107us/step - loss: 37.1903 - mean_absolute_error: 37.1903\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 0s 100us/step - loss: 36.6151 - mean_absolute_error: 36.6151\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 0s 104us/step - loss: 36.6596 - mean_absolute_error: 36.6596\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 36.4672 - mean_absolute_error: 36.4672\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 0s 97us/step - loss: 36.7237 - mean_absolute_error: 36.7237\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 0s 136us/step - loss: 36.4932 - mean_absolute_error: 36.4932\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 0s 112us/step - loss: 36.4234 - mean_absolute_error: 36.4234\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 0s 132us/step - loss: 36.4542 - mean_absolute_error: 36.4542\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 120us/step - loss: 36.3230 - mean_absolute_error: 36.3230\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 36.4217 - mean_absolute_error: 36.4217\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 36.5639 - mean_absolute_error: 36.5639\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 0s 114us/step - loss: 36.2332 - mean_absolute_error: 36.2332\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 0s 96us/step - loss: 36.4355 - mean_absolute_error: 36.4355\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 36.3458 - mean_absolute_error: 36.3458\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 0s 93us/step - loss: 36.5993 - mean_absolute_error: 36.5993\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 0s 95us/step - loss: 36.2160 - mean_absolute_error: 36.2160\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 0s 89us/step - loss: 36.2427 - mean_absolute_error: 36.2427\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 36.5023 - mean_absolute_error: 36.5023\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 0s 95us/step - loss: 36.1723 - mean_absolute_error: 36.1723\n",
      "Epoch 150/500\n",
      "160/160 [==============================] - 0s 95us/step - loss: 36.1669 - mean_absolute_error: 36.1669\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 36.2870 - mean_absolute_error: 36.2870\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 35.9368 - mean_absolute_error: 35.9368\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 0s 94us/step - loss: 36.1463 - mean_absolute_error: 36.1463\n",
      "Epoch 154/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 36.0624 - mean_absolute_error: 36.0624\n",
      "Epoch 155/500\n",
      "160/160 [==============================] - 0s 114us/step - loss: 35.8743 - mean_absolute_error: 35.8743\n",
      "Epoch 156/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 35.8784 - mean_absolute_error: 35.8784\n",
      "Epoch 157/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 35.8952 - mean_absolute_error: 35.8952\n",
      "Epoch 158/500\n",
      "160/160 [==============================] - 0s 98us/step - loss: 36.0439 - mean_absolute_error: 36.0439\n",
      "Epoch 159/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 35.9721 - mean_absolute_error: 35.9721\n",
      "Epoch 160/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 35.8692 - mean_absolute_error: 35.8692\n",
      "Epoch 161/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 35.7825 - mean_absolute_error: 35.7825\n",
      "Epoch 162/500\n",
      "160/160 [==============================] - 0s 97us/step - loss: 35.7826 - mean_absolute_error: 35.7826\n",
      "Epoch 163/500\n",
      "160/160 [==============================] - 0s 107us/step - loss: 35.7630 - mean_absolute_error: 35.7630\n",
      "Epoch 164/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 35.7575 - mean_absolute_error: 35.7575\n",
      "Epoch 165/500\n",
      "160/160 [==============================] - 0s 107us/step - loss: 35.8297 - mean_absolute_error: 35.8297\n",
      "Epoch 166/500\n",
      "160/160 [==============================] - 0s 100us/step - loss: 35.8335 - mean_absolute_error: 35.8335\n",
      "Epoch 167/500\n",
      "160/160 [==============================] - 0s 96us/step - loss: 35.7746 - mean_absolute_error: 35.7746\n",
      "Epoch 168/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 35.8931 - mean_absolute_error: 35.8931\n",
      "Epoch 169/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 36.0646 - mean_absolute_error: 36.0646\n",
      "Epoch 170/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 36.3218 - mean_absolute_error: 36.3218\n",
      "Epoch 171/500\n",
      "160/160 [==============================] - 0s 124us/step - loss: 35.6799 - mean_absolute_error: 35.6799\n",
      "Epoch 172/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 35.7180 - mean_absolute_error: 35.7180\n",
      "Epoch 173/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 35.6907 - mean_absolute_error: 35.6907\n",
      "Epoch 174/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 35.6144 - mean_absolute_error: 35.6144\n",
      "Epoch 175/500\n",
      "160/160 [==============================] - 0s 79us/step - loss: 35.7013 - mean_absolute_error: 35.7013\n",
      "Epoch 176/500\n",
      "160/160 [==============================] - 0s 101us/step - loss: 35.6168 - mean_absolute_error: 35.6168\n",
      "Epoch 177/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 35.3874 - mean_absolute_error: 35.3874\n",
      "Epoch 178/500\n",
      "160/160 [==============================] - 0s 95us/step - loss: 35.4850 - mean_absolute_error: 35.4850\n",
      "Epoch 179/500\n",
      "160/160 [==============================] - 0s 102us/step - loss: 35.4323 - mean_absolute_error: 35.4323\n",
      "Epoch 180/500\n",
      "160/160 [==============================] - 0s 97us/step - loss: 35.3531 - mean_absolute_error: 35.3531\n",
      "Epoch 181/500\n",
      "160/160 [==============================] - 0s 135us/step - loss: 35.4264 - mean_absolute_error: 35.4264\n",
      "Epoch 182/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 35.3655 - mean_absolute_error: 35.3655\n",
      "Epoch 183/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 35.8174 - mean_absolute_error: 35.8174\n",
      "Epoch 184/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 35.7545 - mean_absolute_error: 35.7545\n",
      "Epoch 185/500\n",
      "160/160 [==============================] - 0s 107us/step - loss: 35.4348 - mean_absolute_error: 35.4348\n",
      "Epoch 186/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 35.3484 - mean_absolute_error: 35.3484\n",
      "Epoch 187/500\n",
      "160/160 [==============================] - 0s 103us/step - loss: 35.3273 - mean_absolute_error: 35.3273\n",
      "Epoch 188/500\n",
      "160/160 [==============================] - 0s 103us/step - loss: 35.2824 - mean_absolute_error: 35.2824\n",
      "Epoch 189/500\n",
      "160/160 [==============================] - 0s 96us/step - loss: 35.2861 - mean_absolute_error: 35.2861\n",
      "Epoch 190/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 35.2725 - mean_absolute_error: 35.2725\n",
      "Epoch 191/500\n",
      "160/160 [==============================] - 0s 86us/step - loss: 35.2848 - mean_absolute_error: 35.2848\n",
      "Epoch 192/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 35.1662 - mean_absolute_error: 35.1662\n",
      "Epoch 193/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 35.1553 - mean_absolute_error: 35.1553\n",
      "Epoch 194/500\n",
      "160/160 [==============================] - 0s 153us/step - loss: 35.1551 - mean_absolute_error: 35.1551\n",
      "Epoch 195/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 35.4219 - mean_absolute_error: 35.4219\n",
      "Epoch 196/500\n",
      "160/160 [==============================] - 0s 103us/step - loss: 35.0325 - mean_absolute_error: 35.0325\n",
      "Epoch 197/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 35.1560 - mean_absolute_error: 35.1560\n",
      "Epoch 198/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 35.0677 - mean_absolute_error: 35.0677\n",
      "Epoch 199/500\n",
      "160/160 [==============================] - 0s 152us/step - loss: 35.1804 - mean_absolute_error: 35.1804\n",
      "Epoch 200/500\n",
      "160/160 [==============================] - 0s 128us/step - loss: 35.4816 - mean_absolute_error: 35.4816\n",
      "Epoch 201/500\n",
      "160/160 [==============================] - 0s 129us/step - loss: 35.0536 - mean_absolute_error: 35.0536\n",
      "Epoch 202/500\n",
      "160/160 [==============================] - 0s 132us/step - loss: 35.0523 - mean_absolute_error: 35.0523\n",
      "Epoch 203/500\n",
      "160/160 [==============================] - 0s 133us/step - loss: 34.9915 - mean_absolute_error: 34.9915\n",
      "Epoch 204/500\n",
      "160/160 [==============================] - 0s 124us/step - loss: 35.0243 - mean_absolute_error: 35.0243\n",
      "Epoch 205/500\n",
      "160/160 [==============================] - 0s 107us/step - loss: 35.0242 - mean_absolute_error: 35.0242\n",
      "Epoch 206/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 35.0068 - mean_absolute_error: 35.0068\n",
      "Epoch 207/500\n",
      "160/160 [==============================] - 0s 114us/step - loss: 35.3115 - mean_absolute_error: 35.3115\n",
      "Epoch 208/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 109us/step - loss: 35.1376 - mean_absolute_error: 35.1376\n",
      "Epoch 209/500\n",
      "160/160 [==============================] - 0s 127us/step - loss: 34.9896 - mean_absolute_error: 34.9896\n",
      "Epoch 210/500\n",
      "160/160 [==============================] - 0s 102us/step - loss: 35.1917 - mean_absolute_error: 35.1917\n",
      "Epoch 211/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 34.9949 - mean_absolute_error: 34.9949\n",
      "Epoch 212/500\n",
      "160/160 [==============================] - 0s 97us/step - loss: 35.1627 - mean_absolute_error: 35.1627\n",
      "Epoch 213/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 34.9778 - mean_absolute_error: 34.9778\n",
      "Epoch 214/500\n",
      "160/160 [==============================] - 0s 95us/step - loss: 34.8154 - mean_absolute_error: 34.8154\n",
      "Epoch 215/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 34.7859 - mean_absolute_error: 34.7859\n",
      "Epoch 216/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 34.9589 - mean_absolute_error: 34.9589\n",
      "Epoch 217/500\n",
      "160/160 [==============================] - 0s 93us/step - loss: 34.9752 - mean_absolute_error: 34.9752\n",
      "Epoch 218/500\n",
      "160/160 [==============================] - 0s 88us/step - loss: 35.0711 - mean_absolute_error: 35.0711\n",
      "Epoch 219/500\n",
      "160/160 [==============================] - 0s 127us/step - loss: 34.9853 - mean_absolute_error: 34.9853\n",
      "Epoch 220/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 35.0398 - mean_absolute_error: 35.0398\n",
      "Epoch 221/500\n",
      "160/160 [==============================] - 0s 105us/step - loss: 34.9208 - mean_absolute_error: 34.9208\n",
      "Epoch 222/500\n",
      "160/160 [==============================] - 0s 121us/step - loss: 34.8749 - mean_absolute_error: 34.8749\n",
      "Epoch 223/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 34.6626 - mean_absolute_error: 34.6626\n",
      "Epoch 224/500\n",
      "160/160 [==============================] - 0s 103us/step - loss: 34.7217 - mean_absolute_error: 34.7217\n",
      "Epoch 225/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 34.7212 - mean_absolute_error: 34.7212\n",
      "Epoch 226/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 34.7503 - mean_absolute_error: 34.7503\n",
      "Epoch 227/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 34.6581 - mean_absolute_error: 34.6581\n",
      "Epoch 228/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 34.7609 - mean_absolute_error: 34.7609\n",
      "Epoch 229/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 34.7824 - mean_absolute_error: 34.7824\n",
      "Epoch 230/500\n",
      "160/160 [==============================] - 0s 98us/step - loss: 34.4993 - mean_absolute_error: 34.4993\n",
      "Epoch 231/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 34.7916 - mean_absolute_error: 34.7916\n",
      "Epoch 232/500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 34.7838 - mean_absolute_error: 34.7838\n",
      "Epoch 233/500\n",
      "160/160 [==============================] - 0s 163us/step - loss: 34.7159 - mean_absolute_error: 34.7159\n",
      "Epoch 234/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 34.5595 - mean_absolute_error: 34.5595\n",
      "Epoch 235/500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 34.5587 - mean_absolute_error: 34.5587\n",
      "Epoch 236/500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 34.5126 - mean_absolute_error: 34.5126\n",
      "Epoch 237/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 34.4470 - mean_absolute_error: 34.4470\n",
      "Epoch 238/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 34.4297 - mean_absolute_error: 34.4297\n",
      "Epoch 239/500\n",
      "160/160 [==============================] - 0s 152us/step - loss: 34.4505 - mean_absolute_error: 34.4505\n",
      "Epoch 240/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 34.5599 - mean_absolute_error: 34.5599\n",
      "Epoch 241/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 34.3973 - mean_absolute_error: 34.3973\n",
      "Epoch 242/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 34.4591 - mean_absolute_error: 34.4591\n",
      "Epoch 243/500\n",
      "160/160 [==============================] - 0s 129us/step - loss: 34.3760 - mean_absolute_error: 34.3760\n",
      "Epoch 244/500\n",
      "160/160 [==============================] - 0s 138us/step - loss: 34.3488 - mean_absolute_error: 34.3488\n",
      "Epoch 245/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 34.3102 - mean_absolute_error: 34.3102\n",
      "Epoch 246/500\n",
      "160/160 [==============================] - 0s 141us/step - loss: 34.4084 - mean_absolute_error: 34.4084\n",
      "Epoch 247/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 34.3418 - mean_absolute_error: 34.3418\n",
      "Epoch 248/500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 34.4801 - mean_absolute_error: 34.4801\n",
      "Epoch 249/500\n",
      "160/160 [==============================] - 0s 138us/step - loss: 34.2932 - mean_absolute_error: 34.2932\n",
      "Epoch 250/500\n",
      "160/160 [==============================] - 0s 160us/step - loss: 34.2764 - mean_absolute_error: 34.2764\n",
      "Epoch 251/500\n",
      "160/160 [==============================] - 0s 114us/step - loss: 34.3198 - mean_absolute_error: 34.3198\n",
      "Epoch 252/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 34.3176 - mean_absolute_error: 34.3176\n",
      "Epoch 253/500\n",
      "160/160 [==============================] - 0s 148us/step - loss: 34.2640 - mean_absolute_error: 34.2640\n",
      "Epoch 254/500\n",
      "160/160 [==============================] - 0s 141us/step - loss: 34.2225 - mean_absolute_error: 34.2225\n",
      "Epoch 255/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 34.2887 - mean_absolute_error: 34.2887\n",
      "Epoch 256/500\n",
      "160/160 [==============================] - 0s 146us/step - loss: 34.2265 - mean_absolute_error: 34.2265\n",
      "Epoch 257/500\n",
      "160/160 [==============================] - 0s 130us/step - loss: 34.2067 - mean_absolute_error: 34.2067\n",
      "Epoch 258/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 34.1432 - mean_absolute_error: 34.1432\n",
      "Epoch 259/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 34.2930 - mean_absolute_error: 34.2930\n",
      "Epoch 260/500\n",
      "160/160 [==============================] - 0s 136us/step - loss: 34.2725 - mean_absolute_error: 34.2725\n",
      "Epoch 261/500\n",
      "160/160 [==============================] - 0s 127us/step - loss: 34.1221 - mean_absolute_error: 34.1221\n",
      "Epoch 262/500\n",
      "160/160 [==============================] - 0s 128us/step - loss: 34.1758 - mean_absolute_error: 34.1758\n",
      "Epoch 263/500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 34.1288 - mean_absolute_error: 34.1288\n",
      "Epoch 264/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 34.0231 - mean_absolute_error: 34.0231\n",
      "Epoch 265/500\n",
      "160/160 [==============================] - 0s 129us/step - loss: 34.3118 - mean_absolute_error: 34.3118\n",
      "Epoch 266/500\n",
      "160/160 [==============================] - 0s 156us/step - loss: 34.0978 - mean_absolute_error: 34.0978\n",
      "Epoch 267/500\n",
      "160/160 [==============================] - 0s 160us/step - loss: 34.1151 - mean_absolute_error: 34.1151\n",
      "Epoch 268/500\n",
      "160/160 [==============================] - 0s 145us/step - loss: 33.9717 - mean_absolute_error: 33.9717\n",
      "Epoch 269/500\n",
      "160/160 [==============================] - 0s 127us/step - loss: 34.1026 - mean_absolute_error: 34.1026\n",
      "Epoch 270/500\n",
      "160/160 [==============================] - 0s 102us/step - loss: 34.2045 - mean_absolute_error: 34.2045\n",
      "Epoch 271/500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 34.2255 - mean_absolute_error: 34.2255\n",
      "Epoch 272/500\n",
      "160/160 [==============================] - 0s 135us/step - loss: 34.0565 - mean_absolute_error: 34.0565\n",
      "Epoch 273/500\n",
      "160/160 [==============================] - 0s 136us/step - loss: 34.2443 - mean_absolute_error: 34.2443\n",
      "Epoch 274/500\n",
      "160/160 [==============================] - 0s 131us/step - loss: 33.9474 - mean_absolute_error: 33.9474\n",
      "Epoch 275/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 34.1053 - mean_absolute_error: 34.1053\n",
      "Epoch 276/500\n",
      "160/160 [==============================] - 0s 156us/step - loss: 34.0172 - mean_absolute_error: 34.0172\n",
      "Epoch 277/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 148us/step - loss: 34.0997 - mean_absolute_error: 34.0997\n",
      "Epoch 278/500\n",
      "160/160 [==============================] - 0s 101us/step - loss: 33.9785 - mean_absolute_error: 33.9785\n",
      "Epoch 279/500\n",
      "160/160 [==============================] - 0s 146us/step - loss: 33.9225 - mean_absolute_error: 33.9225\n",
      "Epoch 280/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 34.0712 - mean_absolute_error: 34.0712\n",
      "Epoch 281/500\n",
      "160/160 [==============================] - 0s 153us/step - loss: 34.1256 - mean_absolute_error: 34.1256\n",
      "Epoch 282/500\n",
      "160/160 [==============================] - 0s 161us/step - loss: 33.9220 - mean_absolute_error: 33.9220\n",
      "Epoch 283/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 33.7794 - mean_absolute_error: 33.7794\n",
      "Epoch 284/500\n",
      "160/160 [==============================] - 0s 149us/step - loss: 33.7740 - mean_absolute_error: 33.7740\n",
      "Epoch 285/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 33.8525 - mean_absolute_error: 33.8525\n",
      "Epoch 286/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 33.8117 - mean_absolute_error: 33.8117\n",
      "Epoch 287/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 34.0571 - mean_absolute_error: 34.0571\n",
      "Epoch 288/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 33.8910 - mean_absolute_error: 33.8910\n",
      "Epoch 289/500\n",
      "160/160 [==============================] - 0s 146us/step - loss: 33.7341 - mean_absolute_error: 33.7341\n",
      "Epoch 290/500\n",
      "160/160 [==============================] - 0s 138us/step - loss: 33.7773 - mean_absolute_error: 33.7773\n",
      "Epoch 291/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 34.6374 - mean_absolute_error: 34.6374\n",
      "Epoch 292/500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 33.7809 - mean_absolute_error: 33.7809\n",
      "Epoch 293/500\n",
      "160/160 [==============================] - 0s 158us/step - loss: 33.8434 - mean_absolute_error: 33.8434\n",
      "Epoch 294/500\n",
      "160/160 [==============================] - 0s 138us/step - loss: 33.7767 - mean_absolute_error: 33.7767\n",
      "Epoch 295/500\n",
      "160/160 [==============================] - 0s 142us/step - loss: 33.8195 - mean_absolute_error: 33.8195\n",
      "Epoch 296/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 33.8119 - mean_absolute_error: 33.8119\n",
      "Epoch 297/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 33.7095 - mean_absolute_error: 33.7095\n",
      "Epoch 298/500\n",
      "160/160 [==============================] - 0s 167us/step - loss: 33.6618 - mean_absolute_error: 33.6618\n",
      "Epoch 299/500\n",
      "160/160 [==============================] - 0s 244us/step - loss: 33.6455 - mean_absolute_error: 33.6455\n",
      "Epoch 300/500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 33.8320 - mean_absolute_error: 33.8320\n",
      "Epoch 301/500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 33.6889 - mean_absolute_error: 33.6889\n",
      "Epoch 302/500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 33.6348 - mean_absolute_error: 33.6348\n",
      "Epoch 303/500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 33.6586 - mean_absolute_error: 33.6586\n",
      "Epoch 304/500\n",
      "160/160 [==============================] - 0s 152us/step - loss: 33.6561 - mean_absolute_error: 33.6561\n",
      "Epoch 305/500\n",
      "160/160 [==============================] - 0s 230us/step - loss: 33.7269 - mean_absolute_error: 33.7269\n",
      "Epoch 306/500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 33.7836 - mean_absolute_error: 33.7836\n",
      "Epoch 307/500\n",
      "160/160 [==============================] - 0s 158us/step - loss: 33.6097 - mean_absolute_error: 33.6097\n",
      "Epoch 308/500\n",
      "160/160 [==============================] - 0s 260us/step - loss: 33.6234 - mean_absolute_error: 33.6234\n",
      "Epoch 309/500\n",
      "160/160 [==============================] - 0s 151us/step - loss: 33.8254 - mean_absolute_error: 33.8254\n",
      "Epoch 310/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 33.8278 - mean_absolute_error: 33.8278\n",
      "Epoch 311/500\n",
      "160/160 [==============================] - 0s 155us/step - loss: 34.1056 - mean_absolute_error: 34.1056\n",
      "Epoch 312/500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 33.7299 - mean_absolute_error: 33.7299\n",
      "Epoch 313/500\n",
      "160/160 [==============================] - 0s 142us/step - loss: 33.5364 - mean_absolute_error: 33.5364\n",
      "Epoch 314/500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 33.6137 - mean_absolute_error: 33.6137\n",
      "Epoch 315/500\n",
      "160/160 [==============================] - 0s 146us/step - loss: 33.5489 - mean_absolute_error: 33.5489\n",
      "Epoch 316/500\n",
      "160/160 [==============================] - 0s 165us/step - loss: 33.4797 - mean_absolute_error: 33.4797\n",
      "Epoch 317/500\n",
      "160/160 [==============================] - 0s 146us/step - loss: 33.5313 - mean_absolute_error: 33.5313\n",
      "Epoch 318/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 33.6964 - mean_absolute_error: 33.6964\n",
      "Epoch 319/500\n",
      "160/160 [==============================] - 0s 140us/step - loss: 33.6205 - mean_absolute_error: 33.6205\n",
      "Epoch 320/500\n",
      "160/160 [==============================] - 0s 157us/step - loss: 34.1558 - mean_absolute_error: 34.1558\n",
      "Epoch 321/500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 33.4776 - mean_absolute_error: 33.4776\n",
      "Epoch 322/500\n",
      "160/160 [==============================] - 0s 142us/step - loss: 33.3493 - mean_absolute_error: 33.3493\n",
      "Epoch 323/500\n",
      "160/160 [==============================] - 0s 91us/step - loss: 33.4806 - mean_absolute_error: 33.4806\n",
      "Epoch 324/500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 33.4768 - mean_absolute_error: 33.4768\n",
      "Epoch 325/500\n",
      "160/160 [==============================] - 0s 150us/step - loss: 33.5606 - mean_absolute_error: 33.5606\n",
      "Epoch 326/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 33.3311 - mean_absolute_error: 33.3311\n",
      "Epoch 327/500\n",
      "160/160 [==============================] - 0s 155us/step - loss: 33.7424 - mean_absolute_error: 33.7424\n",
      "Epoch 328/500\n",
      "160/160 [==============================] - 0s 147us/step - loss: 33.4928 - mean_absolute_error: 33.4928\n",
      "Epoch 329/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 33.4818 - mean_absolute_error: 33.4818\n",
      "Epoch 330/500\n",
      "160/160 [==============================] - 0s 129us/step - loss: 33.3766 - mean_absolute_error: 33.3766\n",
      "Epoch 331/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 33.3386 - mean_absolute_error: 33.3386\n",
      "Epoch 332/500\n",
      "160/160 [==============================] - 0s 136us/step - loss: 33.3973 - mean_absolute_error: 33.3973\n",
      "Epoch 333/500\n",
      "160/160 [==============================] - 0s 86us/step - loss: 33.4431 - mean_absolute_error: 33.4431\n",
      "Epoch 334/500\n",
      "160/160 [==============================] - 0s 163us/step - loss: 33.3311 - mean_absolute_error: 33.3311\n",
      "Epoch 335/500\n",
      "160/160 [==============================] - 0s 135us/step - loss: 33.3632 - mean_absolute_error: 33.3632\n",
      "Epoch 336/500\n",
      "160/160 [==============================] - 0s 87us/step - loss: 33.3084 - mean_absolute_error: 33.3084\n",
      "Epoch 337/500\n",
      "160/160 [==============================] - 0s 121us/step - loss: 33.5056 - mean_absolute_error: 33.5056\n",
      "Epoch 338/500\n",
      "160/160 [==============================] - 0s 149us/step - loss: 33.4540 - mean_absolute_error: 33.4540\n",
      "Epoch 339/500\n",
      "160/160 [==============================] - 0s 147us/step - loss: 33.2990 - mean_absolute_error: 33.2990\n",
      "Epoch 340/500\n",
      "160/160 [==============================] - 0s 148us/step - loss: 33.2997 - mean_absolute_error: 33.2997\n",
      "Epoch 341/500\n",
      "160/160 [==============================] - 0s 134us/step - loss: 33.3574 - mean_absolute_error: 33.3574\n",
      "Epoch 342/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 33.9813 - mean_absolute_error: 33.9813\n",
      "Epoch 343/500\n",
      "160/160 [==============================] - 0s 145us/step - loss: 33.5183 - mean_absolute_error: 33.5183\n",
      "Epoch 344/500\n",
      "160/160 [==============================] - 0s 156us/step - loss: 33.4346 - mean_absolute_error: 33.4346\n",
      "Epoch 345/500\n",
      "160/160 [==============================] - 0s 129us/step - loss: 33.3177 - mean_absolute_error: 33.3177\n",
      "Epoch 346/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 139us/step - loss: 33.2208 - mean_absolute_error: 33.2208\n",
      "Epoch 347/500\n",
      "160/160 [==============================] - 0s 142us/step - loss: 33.2246 - mean_absolute_error: 33.2246\n",
      "Epoch 348/500\n",
      "160/160 [==============================] - 0s 103us/step - loss: 33.2237 - mean_absolute_error: 33.2237\n",
      "Epoch 349/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 33.1933 - mean_absolute_error: 33.1933\n",
      "Epoch 350/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 33.1841 - mean_absolute_error: 33.1841\n",
      "Epoch 351/500\n",
      "160/160 [==============================] - 0s 119us/step - loss: 33.2390 - mean_absolute_error: 33.2390\n",
      "Epoch 352/500\n",
      "160/160 [==============================] - 0s 159us/step - loss: 33.4249 - mean_absolute_error: 33.4249\n",
      "Epoch 353/500\n",
      "160/160 [==============================] - 0s 116us/step - loss: 33.1818 - mean_absolute_error: 33.1818\n",
      "Epoch 354/500\n",
      "160/160 [==============================] - 0s 112us/step - loss: 33.2343 - mean_absolute_error: 33.2343\n",
      "Epoch 355/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 33.2440 - mean_absolute_error: 33.2440\n",
      "Epoch 356/500\n",
      "160/160 [==============================] - 0s 102us/step - loss: 33.2092 - mean_absolute_error: 33.2092\n",
      "Epoch 357/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 33.1540 - mean_absolute_error: 33.1540\n",
      "Epoch 358/500\n",
      "160/160 [==============================] - 0s 123us/step - loss: 33.3052 - mean_absolute_error: 33.3052\n",
      "Epoch 359/500\n",
      "160/160 [==============================] - 0s 102us/step - loss: 33.1342 - mean_absolute_error: 33.1342\n",
      "Epoch 360/500\n",
      "160/160 [==============================] - 0s 98us/step - loss: 33.1546 - mean_absolute_error: 33.1546\n",
      "Epoch 361/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 33.1273 - mean_absolute_error: 33.1273\n",
      "Epoch 362/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 33.3188 - mean_absolute_error: 33.3188\n",
      "Epoch 363/500\n",
      "160/160 [==============================] - 0s 108us/step - loss: 33.2603 - mean_absolute_error: 33.2603\n",
      "Epoch 364/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 33.1704 - mean_absolute_error: 33.1704\n",
      "Epoch 365/500\n",
      "160/160 [==============================] - 0s 110us/step - loss: 33.2212 - mean_absolute_error: 33.2212\n",
      "Epoch 366/500\n",
      "160/160 [==============================] - 0s 107us/step - loss: 33.0670 - mean_absolute_error: 33.0670\n",
      "Epoch 367/500\n",
      "160/160 [==============================] - 0s 98us/step - loss: 33.1727 - mean_absolute_error: 33.1727\n",
      "Epoch 368/500\n",
      "160/160 [==============================] - 0s 104us/step - loss: 33.1045 - mean_absolute_error: 33.1045\n",
      "Epoch 369/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 33.1177 - mean_absolute_error: 33.1177\n",
      "Epoch 370/500\n",
      "160/160 [==============================] - 0s 113us/step - loss: 33.0710 - mean_absolute_error: 33.0710\n",
      "Epoch 371/500\n",
      "160/160 [==============================] - 0s 116us/step - loss: 33.1194 - mean_absolute_error: 33.1194\n",
      "Epoch 372/500\n",
      "160/160 [==============================] - 0s 100us/step - loss: 33.2445 - mean_absolute_error: 33.2445\n",
      "Epoch 373/500\n",
      "160/160 [==============================] - 0s 91us/step - loss: 33.0165 - mean_absolute_error: 33.0165\n",
      "Epoch 374/500\n",
      "160/160 [==============================] - 0s 92us/step - loss: 33.0916 - mean_absolute_error: 33.0916\n",
      "Epoch 375/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 33.0029 - mean_absolute_error: 33.0029\n",
      "Epoch 376/500\n",
      "160/160 [==============================] - 0s 100us/step - loss: 33.1746 - mean_absolute_error: 33.1746\n",
      "Epoch 377/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 33.0140 - mean_absolute_error: 33.0140\n",
      "Epoch 378/500\n",
      "160/160 [==============================] - 0s 102us/step - loss: 32.9301 - mean_absolute_error: 32.9301\n",
      "Epoch 379/500\n",
      "160/160 [==============================] - 0s 99us/step - loss: 33.1614 - mean_absolute_error: 33.1614\n",
      "Epoch 380/500\n",
      "160/160 [==============================] - 0s 141us/step - loss: 33.4576 - mean_absolute_error: 33.4576\n",
      "Epoch 381/500\n",
      "160/160 [==============================] - 0s 119us/step - loss: 33.1030 - mean_absolute_error: 33.1030\n",
      "Epoch 382/500\n",
      "160/160 [==============================] - 0s 130us/step - loss: 33.0168 - mean_absolute_error: 33.0168\n",
      "Epoch 383/500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 33.1088 - mean_absolute_error: 33.1088\n",
      "Epoch 384/500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 32.9240 - mean_absolute_error: 32.9240\n",
      "Epoch 385/500\n",
      "160/160 [==============================] - 0s 149us/step - loss: 32.9108 - mean_absolute_error: 32.9108\n",
      "Epoch 386/500\n",
      "160/160 [==============================] - 0s 129us/step - loss: 32.8796 - mean_absolute_error: 32.8796\n",
      "Epoch 387/500\n",
      "160/160 [==============================] - 0s 156us/step - loss: 32.9262 - mean_absolute_error: 32.9262\n",
      "Epoch 388/500\n",
      "160/160 [==============================] - 0s 128us/step - loss: 32.8723 - mean_absolute_error: 32.8723\n",
      "Epoch 389/500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 32.9578 - mean_absolute_error: 32.9578\n",
      "Epoch 390/500\n",
      "160/160 [==============================] - 0s 145us/step - loss: 33.1814 - mean_absolute_error: 33.1814\n",
      "Epoch 391/500\n",
      "160/160 [==============================] - 0s 112us/step - loss: 33.1085 - mean_absolute_error: 33.1085\n",
      "Epoch 392/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 32.8504 - mean_absolute_error: 32.8504\n",
      "Epoch 393/500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 32.9826 - mean_absolute_error: 32.9826\n",
      "Epoch 394/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 32.9191 - mean_absolute_error: 32.9191\n",
      "Epoch 395/500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 33.0184 - mean_absolute_error: 33.0184\n",
      "Epoch 396/500\n",
      "160/160 [==============================] - 0s 145us/step - loss: 32.8893 - mean_absolute_error: 32.8893\n",
      "Epoch 397/500\n",
      "160/160 [==============================] - 0s 158us/step - loss: 33.0966 - mean_absolute_error: 33.0966\n",
      "Epoch 398/500\n",
      "160/160 [==============================] - 0s 143us/step - loss: 32.9827 - mean_absolute_error: 32.9827\n",
      "Epoch 399/500\n",
      "160/160 [==============================] - 0s 148us/step - loss: 32.9108 - mean_absolute_error: 32.9108\n",
      "Epoch 400/500\n",
      "160/160 [==============================] - 0s 149us/step - loss: 33.2341 - mean_absolute_error: 33.2341\n",
      "Epoch 401/500\n",
      "160/160 [==============================] - 0s 159us/step - loss: 32.8845 - mean_absolute_error: 32.8845\n",
      "Epoch 402/500\n",
      "160/160 [==============================] - 0s 105us/step - loss: 32.8880 - mean_absolute_error: 32.8880\n",
      "Epoch 403/500\n",
      "160/160 [==============================] - 0s 157us/step - loss: 32.8878 - mean_absolute_error: 32.8878\n",
      "Epoch 404/500\n",
      "160/160 [==============================] - 0s 124us/step - loss: 33.0042 - mean_absolute_error: 33.0042\n",
      "Epoch 405/500\n",
      "160/160 [==============================] - 0s 123us/step - loss: 32.8894 - mean_absolute_error: 32.8894\n",
      "Epoch 406/500\n",
      "160/160 [==============================] - 0s 139us/step - loss: 32.7553 - mean_absolute_error: 32.7553\n",
      "Epoch 407/500\n",
      "160/160 [==============================] - 0s 127us/step - loss: 32.7847 - mean_absolute_error: 32.7847\n",
      "Epoch 408/500\n",
      "160/160 [==============================] - 0s 141us/step - loss: 32.9012 - mean_absolute_error: 32.9012\n",
      "Epoch 409/500\n",
      "160/160 [==============================] - 0s 96us/step - loss: 32.9583 - mean_absolute_error: 32.9583\n",
      "Epoch 410/500\n",
      "160/160 [==============================] - 0s 121us/step - loss: 32.8246 - mean_absolute_error: 32.8246\n",
      "Epoch 411/500\n",
      "160/160 [==============================] - 0s 153us/step - loss: 32.9557 - mean_absolute_error: 32.9557\n",
      "Epoch 412/500\n",
      "160/160 [==============================] - 0s 165us/step - loss: 32.9480 - mean_absolute_error: 32.9480\n",
      "Epoch 413/500\n",
      "160/160 [==============================] - 0s 128us/step - loss: 33.1727 - mean_absolute_error: 33.1727\n",
      "Epoch 414/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 32.9449 - mean_absolute_error: 32.9449\n",
      "Epoch 415/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 111us/step - loss: 32.6524 - mean_absolute_error: 32.6524\n",
      "Epoch 416/500\n",
      "160/160 [==============================] - 0s 139us/step - loss: 32.7669 - mean_absolute_error: 32.7669\n",
      "Epoch 417/500\n",
      "160/160 [==============================] - 0s 136us/step - loss: 32.7030 - mean_absolute_error: 32.7030\n",
      "Epoch 418/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 32.7206 - mean_absolute_error: 32.7206\n",
      "Epoch 419/500\n",
      "160/160 [==============================] - 0s 97us/step - loss: 32.8193 - mean_absolute_error: 32.8193\n",
      "Epoch 420/500\n",
      "160/160 [==============================] - 0s 153us/step - loss: 33.0685 - mean_absolute_error: 33.0685\n",
      "Epoch 421/500\n",
      "160/160 [==============================] - 0s 123us/step - loss: 32.7293 - mean_absolute_error: 32.7293\n",
      "Epoch 422/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 32.8311 - mean_absolute_error: 32.8311\n",
      "Epoch 423/500\n",
      "160/160 [==============================] - 0s 121us/step - loss: 32.7647 - mean_absolute_error: 32.7647\n",
      "Epoch 424/500\n",
      "160/160 [==============================] - 0s 91us/step - loss: 32.8540 - mean_absolute_error: 32.8540\n",
      "Epoch 425/500\n",
      "160/160 [==============================] - 0s 140us/step - loss: 32.7101 - mean_absolute_error: 32.7101\n",
      "Epoch 426/500\n",
      "160/160 [==============================] - 0s 136us/step - loss: 32.7932 - mean_absolute_error: 32.7932\n",
      "Epoch 427/500\n",
      "160/160 [==============================] - 0s 106us/step - loss: 32.7143 - mean_absolute_error: 32.7143\n",
      "Epoch 428/500\n",
      "160/160 [==============================] - 0s 123us/step - loss: 32.7195 - mean_absolute_error: 32.7195\n",
      "Epoch 429/500\n",
      "160/160 [==============================] - 0s 133us/step - loss: 33.0909 - mean_absolute_error: 33.0909\n",
      "Epoch 430/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 32.7575 - mean_absolute_error: 32.7575\n",
      "Epoch 431/500\n",
      "160/160 [==============================] - 0s 131us/step - loss: 32.6641 - mean_absolute_error: 32.6641\n",
      "Epoch 432/500\n",
      "160/160 [==============================] - 0s 134us/step - loss: 32.7739 - mean_absolute_error: 32.7739\n",
      "Epoch 433/500\n",
      "160/160 [==============================] - 0s 95us/step - loss: 32.6613 - mean_absolute_error: 32.6613\n",
      "Epoch 434/500\n",
      "160/160 [==============================] - 0s 111us/step - loss: 32.6133 - mean_absolute_error: 32.6133\n",
      "Epoch 435/500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 32.7789 - mean_absolute_error: 32.7789\n",
      "Epoch 436/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 33.0349 - mean_absolute_error: 33.0349\n",
      "Epoch 437/500\n",
      "160/160 [==============================] - 0s 151us/step - loss: 32.9206 - mean_absolute_error: 32.9206\n",
      "Epoch 438/500\n",
      "160/160 [==============================] - 0s 135us/step - loss: 32.7597 - mean_absolute_error: 32.7597\n",
      "Epoch 439/500\n",
      "160/160 [==============================] - 0s 109us/step - loss: 32.6692 - mean_absolute_error: 32.6692\n",
      "Epoch 440/500\n",
      "160/160 [==============================] - 0s 136us/step - loss: 32.7404 - mean_absolute_error: 32.7404\n",
      "Epoch 441/500\n",
      "160/160 [==============================] - 0s 145us/step - loss: 32.9049 - mean_absolute_error: 32.9049\n",
      "Epoch 442/500\n",
      "160/160 [==============================] - 0s 121us/step - loss: 32.5416 - mean_absolute_error: 32.5416\n",
      "Epoch 443/500\n",
      "160/160 [==============================] - 0s 130us/step - loss: 32.7468 - mean_absolute_error: 32.7468\n",
      "Epoch 444/500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 32.5082 - mean_absolute_error: 32.5082\n",
      "Epoch 445/500\n",
      "160/160 [==============================] - 0s 115us/step - loss: 32.5632 - mean_absolute_error: 32.5632\n",
      "Epoch 446/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 32.6160 - mean_absolute_error: 32.6160\n",
      "Epoch 447/500\n",
      "160/160 [==============================] - 0s 166us/step - loss: 32.5974 - mean_absolute_error: 32.5974\n",
      "Epoch 448/500\n",
      "160/160 [==============================] - 0s 122us/step - loss: 32.6245 - mean_absolute_error: 32.6245\n",
      "Epoch 449/500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 32.5287 - mean_absolute_error: 32.5287\n",
      "Epoch 450/500\n",
      "160/160 [==============================] - 0s 96us/step - loss: 32.4750 - mean_absolute_error: 32.4750\n",
      "Epoch 451/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 32.5174 - mean_absolute_error: 32.5174\n",
      "Epoch 452/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 32.5456 - mean_absolute_error: 32.5456\n",
      "Epoch 453/500\n",
      "160/160 [==============================] - 0s 134us/step - loss: 32.3896 - mean_absolute_error: 32.3896\n",
      "Epoch 454/500\n",
      "160/160 [==============================] - 0s 117us/step - loss: 32.5396 - mean_absolute_error: 32.5396\n",
      "Epoch 455/500\n",
      "160/160 [==============================] - 0s 100us/step - loss: 32.5915 - mean_absolute_error: 32.5915\n",
      "Epoch 456/500\n",
      "160/160 [==============================] - 0s 134us/step - loss: 32.7245 - mean_absolute_error: 32.7245\n",
      "Epoch 457/500\n",
      "160/160 [==============================] - 0s 126us/step - loss: 32.4162 - mean_absolute_error: 32.4162\n",
      "Epoch 458/500\n",
      "160/160 [==============================] - 0s 132us/step - loss: 32.5256 - mean_absolute_error: 32.5256\n",
      "Epoch 459/500\n",
      "160/160 [==============================] - 0s 125us/step - loss: 32.6279 - mean_absolute_error: 32.6279\n",
      "Epoch 460/500\n",
      "160/160 [==============================] - 0s 139us/step - loss: 32.4653 - mean_absolute_error: 32.4653\n",
      "Epoch 461/500\n",
      "160/160 [==============================] - 0s 116us/step - loss: 32.4830 - mean_absolute_error: 32.4830\n",
      "Epoch 462/500\n",
      "160/160 [==============================] - 0s 150us/step - loss: 32.5542 - mean_absolute_error: 32.5542\n",
      "Epoch 463/500\n",
      "160/160 [==============================] - 0s 118us/step - loss: 32.3895 - mean_absolute_error: 32.3895\n",
      "Epoch 464/500\n",
      "160/160 [==============================] - 0s 144us/step - loss: 32.3720 - mean_absolute_error: 32.3720\n",
      "Epoch 465/500\n",
      "160/160 [==============================] - 0s 128us/step - loss: 32.5401 - mean_absolute_error: 32.5401\n",
      "Epoch 466/500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 32.4780 - mean_absolute_error: 32.4780\n",
      "Epoch 467/500\n",
      "160/160 [==============================] - 0s 151us/step - loss: 32.3973 - mean_absolute_error: 32.3973\n",
      "Epoch 468/500\n",
      "160/160 [==============================] - 0s 102us/step - loss: 32.4414 - mean_absolute_error: 32.4414\n",
      "Epoch 469/500\n",
      "160/160 [==============================] - 0s 240us/step - loss: 32.4833 - mean_absolute_error: 32.4833\n",
      "Epoch 470/500\n",
      "160/160 [==============================] - 0s 105us/step - loss: 32.4597 - mean_absolute_error: 32.4597\n",
      "Epoch 471/500\n",
      "160/160 [==============================] - 0s 235us/step - loss: 32.5243 - mean_absolute_error: 32.5243\n",
      "Epoch 472/500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 32.6515 - mean_absolute_error: 32.6515\n",
      "Epoch 473/500\n",
      "160/160 [==============================] - 0s 262us/step - loss: 32.2858 - mean_absolute_error: 32.2858\n",
      "Epoch 474/500\n",
      "160/160 [==============================] - 0s 222us/step - loss: 32.3596 - mean_absolute_error: 32.3596\n",
      "Epoch 475/500\n",
      "160/160 [==============================] - 0s 146us/step - loss: 32.3587 - mean_absolute_error: 32.3587\n",
      "Epoch 476/500\n",
      "160/160 [==============================] - 0s 135us/step - loss: 32.7465 - mean_absolute_error: 32.7465\n",
      "Epoch 477/500\n",
      "160/160 [==============================] - 0s 138us/step - loss: 32.6201 - mean_absolute_error: 32.6201\n",
      "Epoch 478/500\n",
      "160/160 [==============================] - 0s 148us/step - loss: 32.3280 - mean_absolute_error: 32.3280\n",
      "Epoch 479/500\n",
      "160/160 [==============================] - 0s 137us/step - loss: 32.2553 - mean_absolute_error: 32.2553\n",
      "Epoch 480/500\n",
      "160/160 [==============================] - 0s 130us/step - loss: 32.4390 - mean_absolute_error: 32.4390\n",
      "Epoch 481/500\n",
      "160/160 [==============================] - 0s 149us/step - loss: 32.4631 - mean_absolute_error: 32.4631\n",
      "Epoch 482/500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 32.5897 - mean_absolute_error: 32.5897\n",
      "Epoch 483/500\n",
      "160/160 [==============================] - 0s 127us/step - loss: 32.4156 - mean_absolute_error: 32.4156\n",
      "Epoch 484/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 140us/step - loss: 32.4204 - mean_absolute_error: 32.4204\n",
      "Epoch 485/500\n",
      "160/160 [==============================] - 0s 130us/step - loss: 32.4679 - mean_absolute_error: 32.4679\n",
      "Epoch 486/500\n",
      "160/160 [==============================] - 0s 158us/step - loss: 32.3460 - mean_absolute_error: 32.3460\n",
      "Epoch 487/500\n",
      "160/160 [==============================] - 0s 129us/step - loss: 32.2600 - mean_absolute_error: 32.2600\n",
      "Epoch 488/500\n",
      "160/160 [==============================] - 0s 120us/step - loss: 32.2367 - mean_absolute_error: 32.2367\n",
      "Epoch 489/500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 32.2500 - mean_absolute_error: 32.2500\n",
      "Epoch 490/500\n",
      "160/160 [==============================] - 0s 145us/step - loss: 32.1738 - mean_absolute_error: 32.1738\n",
      "Epoch 491/500\n",
      "160/160 [==============================] - 0s 165us/step - loss: 32.4521 - mean_absolute_error: 32.4521\n",
      "Epoch 492/500\n",
      "160/160 [==============================] - 0s 251us/step - loss: 32.5410 - mean_absolute_error: 32.5410\n",
      "Epoch 493/500\n",
      "160/160 [==============================] - 0s 251us/step - loss: 32.4034 - mean_absolute_error: 32.4034\n",
      "Epoch 494/500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 32.4109 - mean_absolute_error: 32.4109\n",
      "Epoch 495/500\n",
      "160/160 [==============================] - 0s 235us/step - loss: 32.1475 - mean_absolute_error: 32.1475\n",
      "Epoch 496/500\n",
      "160/160 [==============================] - 0s 131us/step - loss: 32.8684 - mean_absolute_error: 32.8684\n",
      "Epoch 497/500\n",
      "160/160 [==============================] - 0s 160us/step - loss: 32.2796 - mean_absolute_error: 32.2796\n",
      "Epoch 498/500\n",
      "160/160 [==============================] - 0s 145us/step - loss: 32.2104 - mean_absolute_error: 32.2104\n",
      "Epoch 499/500\n",
      "160/160 [==============================] - 0s 159us/step - loss: 32.1989 - mean_absolute_error: 32.1989\n",
      "Epoch 500/500\n",
      "160/160 [==============================] - 0s 128us/step - loss: 32.3007 - mean_absolute_error: 32.3007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36829ddcf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,nb_epoch=300,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lM9q-SFqEvLV"
   },
   "outputs": [],
   "source": [
    "# y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIQlB19eFjzj"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# from math import sqrt\n",
    "\n",
    "# rms = sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5WWxm-LF1Kl"
   },
   "outputs": [],
   "source": [
    "# rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "\n",
    "tfjs.converters.save_keras_model(model,\"/home/atharva/Desktop/data science competition/monopoly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "monopoly.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
